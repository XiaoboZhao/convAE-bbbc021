{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt # plotting library\nimport numpy as np # this module is useful to work with numerical arrays\nimport cv2\nimport pandas as pd # this module is useful to work with tabular data\nimport random # this module will be used to select random samples from a collection\nimport os # this module will be used just to create directories in the local filesystem\nfrom tqdm import tqdm # this module is useful to plot progress bars\nimport plotly.io as pio\npio.renderers.default = 'colab'\n\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader,random_split\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport plotly.express as px","metadata":{"id":"hnwiZpM3bVc0","execution":{"iopub.status.busy":"2022-01-04T18:23:15.488883Z","iopub.execute_input":"2022-01-04T18:23:15.489326Z","iopub.status.idle":"2022-01-04T18:23:15.498717Z","shell.execute_reply.started":"2022-01-04T18:23:15.489273Z","shell.execute_reply":"2022-01-04T18:23:15.497758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Import dataset","metadata":{"id":"TE-ayBajboyU"}},{"cell_type":"code","source":"# data_dir = 'dataset'\n# ### With these commands the train and test datasets, respectively, are downloaded\n# ### automatically and stored in the local \"data_dir\" directory.\n# train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n# test_dataset  = torchvision.datasets.MNIST(data_dir, train=False, download=True)","metadata":{"id":"7TQZGuw4bea1","outputId":"7f134736-2259-4eb5-d76d-25347ddaa5ea","execution":{"iopub.status.busy":"2022-01-04T18:23:15.500371Z","iopub.execute_input":"2022-01-04T18:23:15.500839Z","iopub.status.idle":"2022-01-04T18:23:15.511309Z","shell.execute_reply.started":"2022-01-04T18:23:15.500801Z","shell.execute_reply":"2022-01-04T18:23:15.510601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms_train = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((220, 220)),\n    transforms.ToTensor(),\n#     transforms.Normalize((0.1640, 0.1484, 0.1410), (0.0894, 0.1212, 0.1265))\n])\n\ntransforms_test = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((220, 220)),\n    transforms.ToTensor(),\n#     transforms.Normalize((0.1640, 0.1484, 0.1410), (0.0894, 0.1212, 0.1265))\n#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nsim_df = pd.read_csv('../input/images-bbbc021/similarity_1400_label.csv')\nbase_dir = '../input/images-bbbc021/images_bbbc021/images_bbbc021/'\ndf_len = len(sim_df)\n\nfor i in range(df_len):\n    sim_df.iloc[i, 0] = 'bbbc021_' + str(sim_df.iloc[i, 0]) + '.png'\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T18:23:15.512542Z","iopub.execute_input":"2022-01-04T18:23:15.512873Z","iopub.status.idle":"2022-01-04T18:23:16.949089Z","shell.execute_reply.started":"2022-01-04T18:23:15.512811Z","shell.execute_reply":"2022-01-04T18:23:16.948332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Our own custom class for datasets\nclass CreateDataset(Dataset):\n    def __init__(self, df_data, data_dir=base_dir, transform=None):\n        super().__init__()\n        self.df = df_data.values\n        self.data_dir = data_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n#         img_name,label = self.df[index]\n        img_name, label = self.df[index]\n\n#         target = torch.zeros(nb_classes)\n#         target[label] = 1\n        \n#         img_path = os.path.join(self.data_dir, img_name+'.tif')\n        img_path = os.path.join(self.data_dir, img_name)\n        image = cv2.imread(img_path)\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2022-01-04T18:23:16.950318Z","iopub.execute_input":"2022-01-04T18:23:16.95058Z","iopub.status.idle":"2022-01-04T18:23:16.960019Z","shell.execute_reply.started":"2022-01-04T18:23:16.950546Z","shell.execute_reply":"2022-01-04T18:23:16.959281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=64\n\ntrain_df = sim_df.sample(frac=0.8, random_state=1)\nval_df = sim_df.drop(train_df.index)\n\ntest_dataset = CreateDataset(df_data=val_df, data_dir=base_dir)\n\ntrain_data = CreateDataset(df_data=train_df, data_dir=base_dir, transform=transforms_train)\nval_data = CreateDataset(df_data=val_df, data_dir=base_dir, transform=transforms_test)\ntrain_loader = DataLoader(train_data, batch_size=batch_size)\nvalid_loader = DataLoader(val_data, batch_size=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T18:23:16.963246Z","iopub.execute_input":"2022-01-04T18:23:16.96357Z","iopub.status.idle":"2022-01-04T18:23:16.975983Z","shell.execute_reply.started":"2022-01-04T18:23:16.963532Z","shell.execute_reply":"2022-01-04T18:23:16.975146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(5, 5, figsize=(8,8))\nfor ax in axs.flatten():\n    # random.choice allows to randomly sample from a list-like object (basically anything that can be accessed with an index, like our dataset)\n    img, label = random.choice(CreateDataset(df_data=train_df, data_dir=base_dir))\n    ax.imshow(np.array(img))\n    ax.set_title('Label: %d' % label)\n    ax.set_xticks([])\n    ax.set_yticks([])\nplt.tight_layout()","metadata":{"id":"T69RbK5Kbeda","outputId":"4de6b53c-3e76-4a85-f517-64526601d4f9","execution":{"iopub.status.busy":"2022-01-04T18:23:16.977459Z","iopub.execute_input":"2022-01-04T18:23:16.977918Z","iopub.status.idle":"2022-01-04T18:23:19.26766Z","shell.execute_reply.started":"2022-01-04T18:23:16.977885Z","shell.execute_reply":"2022-01-04T18:23:19.265854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # test\n# img","metadata":{"execution":{"iopub.status.busy":"2022-01-04T18:23:19.269017Z","iopub.execute_input":"2022-01-04T18:23:19.269635Z","iopub.status.idle":"2022-01-04T18:23:19.273441Z","shell.execute_reply.started":"2022-01-04T18:23:19.269597Z","shell.execute_reply":"2022-01-04T18:23:19.272687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_transform = transforms.Compose([\n#     transforms.ToTensor(),\n# ])\n# test_transform = transforms.Compose([\n#     transforms.ToTensor(),\n# ])\n\n# # Set the train transform\n# train_dataset.transform = train_transform\n# # Set the test transform\n# test_dataset.transform = test_transform","metadata":{"id":"1OQt6spHbegf","execution":{"iopub.status.busy":"2022-01-04T18:23:19.27475Z","iopub.execute_input":"2022-01-04T18:23:19.274996Z","iopub.status.idle":"2022-01-04T18:23:19.283397Z","shell.execute_reply.started":"2022-01-04T18:23:19.274964Z","shell.execute_reply":"2022-01-04T18:23:19.28258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# m=len(train_dataset)\n\n#random_split randomly split a dataset into non-overlapping new datasets of given lengths\n#train (55,000 images), val split (5,000 images)\n# train_data, val_data = random_split(train_dataset, [int(m-m*0.2), int(m*0.2)])\n\n# batch_size=256\n\n# The dataloaders handle shuffling, batching, etc...\n# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n# valid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n# # test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=True)","metadata":{"id":"J59qPwtvbiEs","execution":{"iopub.status.busy":"2022-01-04T18:23:19.284896Z","iopub.execute_input":"2022-01-04T18:23:19.285157Z","iopub.status.idle":"2022-01-04T18:23:19.291421Z","shell.execute_reply.started":"2022-01-04T18:23:19.285123Z","shell.execute_reply":"2022-01-04T18:23:19.290523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Define Convolutional Autoencoder","metadata":{"id":"6opjhsCCbs4t"}},{"cell_type":"code","source":"# # test\n# m = nn.Conv2d(3, 8, 3, stride=2, padding=1)\n# # # non-square kernels and unequal stride and with padding\n# # m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n# # # non-square kernels and unequal stride and with padding and dilation\n# # m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n# input = torch.randn(20, 3, 220, 220)\n# output = m(input)\n# output = nn.Conv2d(8, 16, 3, stride=2, padding=1)(output)\n# output = nn.Conv2d(16, 32, 3, stride=2, padding=0)(output)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T18:23:19.293059Z","iopub.execute_input":"2022-01-04T18:23:19.293321Z","iopub.status.idle":"2022-01-04T18:23:19.301351Z","shell.execute_reply.started":"2022-01-04T18:23:19.29329Z","shell.execute_reply":"2022-01-04T18:23:19.300653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-04T18:23:19.302794Z","iopub.execute_input":"2022-01-04T18:23:19.303347Z","iopub.status.idle":"2022-01-04T18:23:19.310012Z","shell.execute_reply.started":"2022-01-04T18:23:19.303313Z","shell.execute_reply":"2022-01-04T18:23:19.309097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \n    def __init__(self, encoded_space_dim,fc2_input_dim):\n        super().__init__()\n        \n        ### Convolutional section\n        self.encoder_cnn = nn.Sequential(\n            # First convolutional layer\n            nn.Conv2d(3, 8, 3, stride=2, padding=1),\n            #nn.BatchNorm2d(8),\n            nn.ReLU(True),\n            # Second convolutional layer\n            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n            nn.BatchNorm2d(16),\n            nn.ReLU(True),\n            # Third convolutional layer\n            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n            #nn.BatchNorm2d(32),\n            nn.ReLU(True)\n        )\n        \n        ### Flatten layer\n        self.flatten = nn.Flatten(start_dim=1)\n\n        ### Linear section\n        self.encoder_lin = nn.Sequential(\n            # First linear layer\n            nn.Linear(27 * 27 * 32, 128),\n            nn.ReLU(True),\n            # Second linear layer\n            nn.Linear(128, encoded_space_dim)\n        )\n        \n    def forward(self, x):\n        # Apply convolutions\n        x = self.encoder_cnn(x)\n        # Flatten\n        x = self.flatten(x)\n        # # Apply linear layers\n        x = self.encoder_lin(x)\n        return x","metadata":{"id":"vt2xa81JbiHm","execution":{"iopub.status.busy":"2022-01-04T18:23:19.311482Z","iopub.execute_input":"2022-01-04T18:23:19.31174Z","iopub.status.idle":"2022-01-04T18:23:19.322172Z","shell.execute_reply.started":"2022-01-04T18:23:19.311702Z","shell.execute_reply":"2022-01-04T18:23:19.321361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    \n    def __init__(self, encoded_space_dim,fc2_input_dim):\n        super().__init__()\n\n        ### Linear section\n        self.decoder_lin = nn.Sequential(\n            # First linear layer\n            nn.Linear(encoded_space_dim, 128),\n            nn.ReLU(True),\n            # Second linear layer\n            nn.Linear(128, 27 * 27 * 32),\n            nn.ReLU(True)\n        )\n\n        ### Unflatten\n        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 27, 27))\n\n        ### Convolutional section\n        self.decoder_conv = nn.Sequential(\n            # First transposed convolution\n            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n            nn.BatchNorm2d(16),\n            nn.ReLU(True),\n            # Second transposed convolution\n            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n            nn.BatchNorm2d(8),\n            nn.ReLU(True),\n            # Third transposed convolution\n            nn.ConvTranspose2d(8, 3, 3, stride=2, padding=1, output_padding=1)\n        )\n        \n    def forward(self, x):\n        # Apply linear layers\n        x = self.decoder_lin(x)\n        # Unflatten\n        x = self.unflatten(x)\n        # Apply transposed convolutions\n        x = self.decoder_conv(x)\n        # Apply a sigmoid to force the output to be between 0 and 1 (valid pixel values)\n        x = torch.sigmoid(x)\n        return x","metadata":{"id":"jOEDa8pQbiKR","execution":{"iopub.status.busy":"2022-01-04T18:23:19.323382Z","iopub.execute_input":"2022-01-04T18:23:19.323934Z","iopub.status.idle":"2022-01-04T18:23:19.335911Z","shell.execute_reply.started":"2022-01-04T18:23:19.323897Z","shell.execute_reply":"2022-01-04T18:23:19.33508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Initialize Loss function and the optimizer","metadata":{"id":"bFo50k3cb3e8"}},{"cell_type":"code","source":"### Set the random seed for reproducible results\ntorch.manual_seed(0)\n\n### Initialize the two networks\nd = 64\n\n#model = Autoencoder(encoded_space_dim=encoded_space_dim)\nencoder = Encoder(encoded_space_dim=d,fc2_input_dim=128)\ndecoder = Decoder(encoded_space_dim=d,fc2_input_dim=128)","metadata":{"id":"DR1aV3Xeb52H","execution":{"iopub.status.busy":"2022-01-04T18:23:19.33954Z","iopub.execute_input":"2022-01-04T18:23:19.339801Z","iopub.status.idle":"2022-01-04T18:23:19.39559Z","shell.execute_reply.started":"2022-01-04T18:23:19.339768Z","shell.execute_reply":"2022-01-04T18:23:19.394799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Some examples\n# Take an input image (remember to add the batch dimension)\nimg, _ = val_data[0]\nimg = img.unsqueeze(0) # Add the batch dimension in the first axis\nprint('Original image shape:', img.shape)\n# Encode the image\n\nimg_enc = encoder(img)\nprint('Encoded image shape:', img_enc.shape)\n\n# Decode the image\ndec_img = decoder(img_enc)\n#dec_img = model(img)\nprint('Decoded image shape:', dec_img.shape)","metadata":{"id":"-lbs0kwPb54_","outputId":"11d2e190-fc55-497c-d787-540c16e9ecd4","execution":{"iopub.status.busy":"2022-01-04T18:23:19.39676Z","iopub.execute_input":"2022-01-04T18:23:19.397024Z","iopub.status.idle":"2022-01-04T18:23:19.435362Z","shell.execute_reply.started":"2022-01-04T18:23:19.396989Z","shell.execute_reply":"2022-01-04T18:23:19.434458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Define the loss function\nloss_fn = torch.nn.MSELoss()\n\n### Define an optimizer (both for the encoder and the decoder!)\nlr= 0.001\n#lr = 0.0008 # Learning rate\n\n\nparams_to_optimize = [\n    {'params': encoder.parameters()},\n    {'params': decoder.parameters()}\n]\n\noptim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n#optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=6e-05)\n\n# Check if the GPU is available\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(f'Selected device: {device}')\n\n# Move both the encoder and the decoder to the selected device\nencoder.to(device)\ndecoder.to(device)\n#model.to(device)","metadata":{"id":"5Bp86OU5b58J","outputId":"6c427269-c93d-44a3-b4b0-9deeb7b48760","execution":{"iopub.status.busy":"2022-01-04T18:23:19.439057Z","iopub.execute_input":"2022-01-04T18:23:19.439323Z","iopub.status.idle":"2022-01-04T18:23:19.459683Z","shell.execute_reply.started":"2022-01-04T18:23:19.439292Z","shell.execute_reply":"2022-01-04T18:23:19.458715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Train and evaluate model","metadata":{"id":"ohtcy0rScA91"}},{"cell_type":"code","source":"### Training function\ndef train_epoch(encoder, decoder, device, dataloader, loss_fn, optimizer):\n    # Set train mode for both the encoder and the decoder\n    encoder.train()\n    decoder.train()\n    train_loss = []\n    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n    for image_batch, _ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n        # Move tensor to the proper device\n        image_batch = image_batch.to(device)\n        # Encode data\n        encoded_data = encoder(image_batch)\n        # Decode data\n        decoded_data = decoder(encoded_data)\n        # Evaluate loss\n        loss = loss_fn(decoded_data, image_batch)\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # Print batch loss\n        print('\\t partial train loss (single batch): %f' % (loss.data))\n        train_loss.append(loss.detach().cpu().numpy())\n\n    return np.mean(train_loss)","metadata":{"id":"Lm8I37QBcAZd","execution":{"iopub.status.busy":"2022-01-04T18:23:19.461397Z","iopub.execute_input":"2022-01-04T18:23:19.461701Z","iopub.status.idle":"2022-01-04T18:23:19.470331Z","shell.execute_reply.started":"2022-01-04T18:23:19.461662Z","shell.execute_reply":"2022-01-04T18:23:19.46925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Testing function\ndef test_epoch(encoder, decoder, device, dataloader, loss_fn):\n    # Set evaluation mode for encoder and decoder\n    encoder.eval()\n    decoder.eval()\n    with torch.no_grad(): # No need to track the gradients\n        # Define the lists to store the outputs for each batch\n        conc_out = []\n        conc_label = []\n        for image_batch, _ in dataloader:\n            # Move tensor to the proper device\n            image_batch = image_batch.to(device)\n            # Encode data\n            encoded_data = encoder(image_batch)\n            # Decode data\n            decoded_data = decoder(encoded_data)\n            # Append the network output and the original image to the lists\n            conc_out.append(decoded_data.cpu())\n            conc_label.append(image_batch.cpu())\n        # Create a single tensor with all the values in the lists\n        conc_out = torch.cat(conc_out)\n        conc_label = torch.cat(conc_label) \n        # Evaluate global loss\n        val_loss = loss_fn(conc_out, conc_label)\n    return val_loss.data","metadata":{"id":"m1P4xDmecLGu","execution":{"iopub.status.busy":"2022-01-04T18:23:19.472603Z","iopub.execute_input":"2022-01-04T18:23:19.473011Z","iopub.status.idle":"2022-01-04T18:23:19.484123Z","shell.execute_reply.started":"2022-01-04T18:23:19.472884Z","shell.execute_reply":"2022-01-04T18:23:19.483138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_ae_outputs(encoder,decoder,n=5):\n    plt.figure(figsize=(10,4.5))\n    for i in range(n):\n        ax = plt.subplot(2,n,i+1)\n        img = val_data[i][0].unsqueeze(0).to(device)\n        \n#         print(img)\n#         print(img.shape)\n        \n        encoder.eval()\n        decoder.eval()\n        with torch.no_grad():\n            rec_img = decoder(encoder(img))\n        img = img.cpu().squeeze().numpy().transpose(1, 2, 0)\n        \n#         print(img.shape)\n        \n        plt.imshow(img)\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)  \n        if i == n//2:\n            ax.set_title('Original images')\n        ax = plt.subplot(2, n, i + 1 + n)\n        rec_img = rec_img.cpu().squeeze().numpy().transpose(1, 2, 0)\n        \n#         print(rec_img.shape)\n        \n        plt.imshow(rec_img)\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)  \n        if i == n//2:\n            ax.set_title('Reconstructed images')\n    plt.show()   ","metadata":{"id":"YNt-JSk4cNAV","execution":{"iopub.status.busy":"2022-01-04T18:23:19.486469Z","iopub.execute_input":"2022-01-04T18:23:19.486863Z","iopub.status.idle":"2022-01-04T18:23:19.500219Z","shell.execute_reply.started":"2022-01-04T18:23:19.486799Z","shell.execute_reply":"2022-01-04T18:23:19.499227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 30\nhistory={'train_loss':[],'val_loss':[]}\nfor epoch in range(num_epochs):\n\n   train_loss = train_epoch(encoder,decoder,device,train_loader,loss_fn,optim)\n   val_loss = test_epoch(encoder,decoder,device,valid_loader,loss_fn)\n   print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n   history['train_loss'].append(train_loss)\n   history['val_loss'].append(val_loss)\n   plot_ae_outputs(encoder,decoder,n=5)","metadata":{"id":"xjDZ51gccOoN","outputId":"16a684f5-077b-457d-ad0e-19c04bef8348","execution":{"iopub.status.busy":"2022-01-04T18:23:19.502203Z","iopub.execute_input":"2022-01-04T18:23:19.502812Z","iopub.status.idle":"2022-01-04T19:00:14.856793Z","shell.execute_reply.started":"2022-01-04T18:23:19.502767Z","shell.execute_reply":"2022-01-04T19:00:14.856149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_epoch(encoder,decoder,device,test_loader,loss_fn).item()","metadata":{"id":"-tdZTtU1cTgS","outputId":"b36c518d-0e96-44ba-d15d-23f05299e42b","execution":{"iopub.status.busy":"2022-01-04T17:13:21.395993Z","iopub.execute_input":"2022-01-04T17:13:21.396257Z","iopub.status.idle":"2022-01-04T17:13:21.418015Z","shell.execute_reply.started":"2022-01-04T17:13:21.396227Z","shell.execute_reply":"2022-01-04T17:13:21.417121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_epoch(encoder,decoder,device,test_loader,loss_fn).item()# Plot losses\nplt.figure(figsize=(10,8))\nplt.semilogy(history['train_loss'], label='Train')\nplt.semilogy(history['val_loss'], label='Valid')\nplt.xlabel('Epoch')\nplt.ylabel('Average Loss')\n#plt.grid()\nplt.legend()\n#plt.title('loss')\nplt.show()","metadata":{"id":"-qtRq37scQg2","outputId":"7cf914ea-51af-464e-fd28-1a8574e876bd","execution":{"iopub.status.busy":"2022-01-04T09:30:14.955621Z","iopub.execute_input":"2022-01-04T09:30:14.956039Z","iopub.status.idle":"2022-01-04T09:30:16.296134Z","shell.execute_reply.started":"2022-01-04T09:30:14.956001Z","shell.execute_reply":"2022-01-04T09:30:16.29546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Generate new samples from the latent code","metadata":{"id":"G_xKYOjRcVaX"}},{"cell_type":"code","source":"def plot_reconstructed(decoder, r0=(-5, 10), r1=(-10, 5), n=10):\n    plt.figure(figsize=(20,8.5))\n    w = 28\n    img = np.zeros((n*w, n*w))\n    for i, y in enumerate(np.linspace(*r1, n)):\n        for j, x in enumerate(np.linspace(*r0, n)):\n            z = torch.Tensor([[x, y]]).to(device)\n            x_hat = decoder(z)\n            x_hat = x_hat.reshape(28, 28).to('cpu').detach().numpy()\n            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat\n    plt.imshow(img, extent=[*r0, *r1], cmap='gist_gray')","metadata":{"id":"Zmkf9D9YcQj2","execution":{"iopub.status.busy":"2022-01-04T09:30:16.297404Z","iopub.execute_input":"2022-01-04T09:30:16.297801Z","iopub.status.idle":"2022-01-04T09:30:16.307552Z","shell.execute_reply.started":"2022-01-04T09:30:16.297766Z","shell.execute_reply":"2022-01-04T09:30:16.306812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_reconstructed(decoder, r0=(-1, 1), r1=(-1, 1))","metadata":{"id":"JKyx8AmpccNG","outputId":"968b6b1f-c0f1-4b80-e059-475a0d2f3677","execution":{"iopub.status.busy":"2022-01-04T09:30:16.308818Z","iopub.execute_input":"2022-01-04T09:30:16.309064Z","iopub.status.idle":"2022-01-04T09:30:16.409792Z","shell.execute_reply.started":"2022-01-04T09:30:16.309031Z","shell.execute_reply":"2022-01-04T09:30:16.408716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Visualize Latent Code ","metadata":{"id":"Ncwen_YdcfQe"}},{"cell_type":"code","source":"encoded_samples = []\nfor sample in tqdm(test_dataset):\n    img = sample[0].unsqueeze(0).to(device)\n    label = sample[1]\n    # Encode image\n    encoder.eval()\n    with torch.no_grad():\n        encoded_img  = encoder(img)\n    # Append to list\n    encoded_img = encoded_img.flatten().cpu().numpy()\n    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n    encoded_sample['label'] = label\n    encoded_samples.append(encoded_sample)","metadata":{"id":"KfgIoJeMcl2-","outputId":"5661342d-1243-424f-f1ba-6ab4ef43b516","execution":{"iopub.status.busy":"2022-01-04T09:31:24.896417Z","iopub.execute_input":"2022-01-04T09:31:24.896696Z","iopub.status.idle":"2022-01-04T09:31:32.971339Z","shell.execute_reply.started":"2022-01-04T09:31:24.896647Z","shell.execute_reply":"2022-01-04T09:31:32.970421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_samples = pd.DataFrame(encoded_samples)\nencoded_samples","metadata":{"id":"a4M-taNNcl5m","outputId":"5de3a279-c6f4-4890-e228-c75b8023fabb","execution":{"iopub.status.busy":"2022-01-04T09:31:35.721233Z","iopub.execute_input":"2022-01-04T09:31:35.721483Z","iopub.status.idle":"2022-01-04T09:31:35.775224Z","shell.execute_reply.started":"2022-01-04T09:31:35.721454Z","shell.execute_reply":"2022-01-04T09:31:35.774522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.scatter(encoded_samples, x='Enc. Variable 0', y='Enc. Variable 1', color=encoded_samples.label.astype(str), opacity=0.7)","metadata":{"id":"LBEWZ1zLcOrk","outputId":"9c430ec4-e3a4-4753-d5c0-b9e96c9da83f","execution":{"iopub.status.busy":"2022-01-04T09:32:04.59205Z","iopub.execute_input":"2022-01-04T09:32:04.592297Z","iopub.status.idle":"2022-01-04T09:32:04.718735Z","shell.execute_reply.started":"2022-01-04T09:32:04.592269Z","shell.execute_reply":"2022-01-04T09:32:04.717864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(17, 9))\nplt.scatter(encoded_samples['Enc. Variable 0'], encoded_samples['Enc. Variable 1'], c=encoded_samples.label, cmap='tab10')\nplt.colorbar()\nplt.show()","metadata":{"id":"XuxP7S8AcNDe","outputId":"d62d93bc-ece2-4452-ca59-7297ac76caac","execution":{"iopub.status.busy":"2022-01-04T09:32:13.588709Z","iopub.execute_input":"2022-01-04T09:32:13.589256Z","iopub.status.idle":"2022-01-04T09:32:14.028884Z","shell.execute_reply.started":"2022-01-04T09:32:13.589218Z","shell.execute_reply":"2022-01-04T09:32:14.025931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=2)\ncomponents = pca.fit_transform(encoded_samples.drop(['label'],axis=1))\ntotal_var = pca.explained_variance_ratio_.sum() * 100\nprint(total_var)","metadata":{"id":"jJ5h_K5wcNGU","outputId":"4ad354a8-8ec8-4098-a077-0a32849962e5","execution":{"iopub.status.busy":"2022-01-04T09:32:20.311903Z","iopub.execute_input":"2022-01-04T09:32:20.312435Z","iopub.status.idle":"2022-01-04T09:32:20.360689Z","shell.execute_reply.started":"2022-01-04T09:32:20.312397Z","shell.execute_reply":"2022-01-04T09:32:20.359849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter(components, x=0, y=1, color=encoded_samples.label.astype(str),labels={'0': 'PC 1', '1': 'PC 2'})\nfig.show()","metadata":{"id":"YIz9Bcz0cLJ9","outputId":"166e5fe6-f13f-4831-e208-4e2708d8bc7f","execution":{"iopub.status.busy":"2022-01-04T09:32:23.32094Z","iopub.execute_input":"2022-01-04T09:32:23.321197Z","iopub.status.idle":"2022-01-04T09:32:23.443872Z","shell.execute_reply.started":"2022-01-04T09:32:23.321166Z","shell.execute_reply":"2022-01-04T09:32:23.443116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(17, 9))\nplt.scatter(components[:,0], components[:,1], c=encoded_samples.label, cmap='tab10')\nplt.colorbar()\nplt.show()","metadata":{"id":"pgAHzOkocAcs","outputId":"4e4dd2ec-6fb6-4943-8d0f-da277f32cb5c","execution":{"iopub.status.busy":"2022-01-04T09:32:45.507913Z","iopub.execute_input":"2022-01-04T09:32:45.508161Z","iopub.status.idle":"2022-01-04T09:32:46.141137Z","shell.execute_reply.started":"2022-01-04T09:32:45.508134Z","shell.execute_reply":"2022-01-04T09:32:46.140507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne = TSNE(n_components=2)\ntsne_results = tsne.fit_transform(encoded_samples.drop(['label'],axis=1))","metadata":{"id":"iG65lKkocAhs","execution":{"iopub.status.busy":"2022-01-04T09:32:51.482031Z","iopub.execute_input":"2022-01-04T09:32:51.482595Z","iopub.status.idle":"2022-01-04T09:33:33.521529Z","shell.execute_reply.started":"2022-01-04T09:32:51.482554Z","shell.execute_reply":"2022-01-04T09:33:33.520611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter(tsne_results, x=0, y=1, color=encoded_samples.label.astype(str),labels={'0': 'tsne-2d-one', '1': 'tsne-2d-two'})\nfig.show()","metadata":{"id":"0_wbvT-tcAlD","outputId":"9a89b374-67b3-401f-f5e1-4c53a2e3c7a7","execution":{"iopub.status.busy":"2022-01-04T09:33:37.280222Z","iopub.execute_input":"2022-01-04T09:33:37.280983Z","iopub.status.idle":"2022-01-04T09:33:37.304696Z","shell.execute_reply.started":"2022-01-04T09:33:37.28092Z","shell.execute_reply":"2022-01-04T09:33:37.303705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(17, 9))\nplt.scatter(tsne_results[:,0], tsne_results[:,1], c=encoded_samples.label, cmap='tab10')\nplt.colorbar()\nplt.show()","metadata":{"id":"fdHu-fSkbej5","outputId":"a63b0d79-7cfb-4e10-a77e-fd444662af13","execution":{"iopub.status.busy":"2022-01-04T09:33:39.69894Z","iopub.execute_input":"2022-01-04T09:33:39.699197Z","iopub.status.idle":"2022-01-04T09:33:39.72547Z","shell.execute_reply.started":"2022-01-04T09:33:39.699168Z","shell.execute_reply":"2022-01-04T09:33:39.724485Z"},"trusted":true},"execution_count":null,"outputs":[]}]}